{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "cells": [
  {
   "id": "cell-01",
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Spam Email Classifier\n\nEnd-to-end pipeline: download \u2192 preprocess \u2192 train \u2192 evaluate \u2192 compare.\n\n| Model | Architecture |\n|---|---|\n| **Baseline** | TF-IDF (uni + bigrams) \u2192 Logistic Regression |\n| **Advanced** | Learned Embedding \u2192 Bi-directional LSTM |\n\n**Dataset:** [abdallahwagih/spam-emails](https://www.kaggle.com/datasets/abdallahwagih/spam-emails) (Kaggle)\n**Libraries:** scikit-learn \u00b7 PyTorch \u00b7 pandas \u00b7 matplotlib \u00b7 seaborn\n"
  },
  {
   "id": "cell-02",
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "%matplotlib inline\nimport sys, os\n\n# Ensure project root is on the path so config, preprocess, etc. are importable\nsys.path.insert(0, os.getcwd())\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nplt.rcParams.update({\"figure.dpi\": 100, \"figure.figsize\": (8, 5)})\n"
  },
  {
   "id": "cell-03",
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 1  Download & Explore the Dataset\n\nThe dataset is fetched from Kaggle via `kagglehub`. It contains SMS messages\nlabelled *ham* (legitimate) or *spam*. After de-duplication we get **5 572 messages**.\n\nThe data is split with **stratification** so every subset keeps the same\n\u2248 13 % spam / 87 % ham ratio.\n\n> **First run:** requires a Kaggle account and an active `kagglehub` session.\n> Subsequent runs use the cached copy.\n"
  },
  {
   "id": "cell-04",
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from download_data import download_and_extract, load_and_clean, split_and_save\n\ndataset_path = download_and_extract()\ndf           = load_and_clean(dataset_path)\nsplit_and_save(df)\n"
  },
  {
   "id": "cell-05",
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from config import DATA_DIR\n\ntrain_df = pd.read_csv(os.path.join(DATA_DIR, \"train.csv\"))\nval_df   = pd.read_csv(os.path.join(DATA_DIR, \"validation.csv\"))\ntest_df  = pd.read_csv(os.path.join(DATA_DIR, \"test.csv\"))\n\n# Summary\nprint(f\"{'Split':<14} {'Rows':>6}   {'Spam %':>7}\")\nprint(\"-\" * 32)\nfor name, split in [(\"Train\", train_df), (\"Validation\", val_df), (\"Test\", test_df)]:\n    print(f\"{name:<14} {len(split):>6}   {split['label'].mean()*100:>6.1f} %\")\n\n# Sample rows rendered as an HTML table\nsample = train_df.sample(4, random_state=42).copy()\nsample[\"label\"] = sample[\"label\"].map({0: \"ham\", 1: \"spam\"})\nsample[[\"label\", \"text\"]]\n"
  },
  {
   "id": "cell-06",
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "labels  = [\"Ham\", \"Spam\"]\ncolors  = [\"steelblue\", \"coral\"]\nsplits  = [(\"Train\", train_df), (\"Validation\", val_df), (\"Test\", test_df)]\n\nfig, axes = plt.subplots(1, 3, figsize=(12, 3.5))\nfor ax, (name, split) in zip(axes, splits):\n    counts = split[\"label\"].value_counts().sort_index()\n    ax.bar(labels, counts.values, color=colors, edgecolor=\"white\")\n    ax.set_title(name, fontsize=12)\n    ax.set_ylabel(\"Count\")\n    for i, v in enumerate(counts.values):\n        ax.text(i, v + 15, str(v), ha=\"center\", fontsize=10)\n    ax.grid(axis=\"y\", alpha=0.3)\n\nplt.suptitle(\"Class Distribution Across Splits\", fontsize=13, y=1.02)\nplt.tight_layout()\nplt.show()\n"
  },
  {
   "id": "cell-07",
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 2  Text Preprocessing\n\nEvery message goes through the same six-step pipeline **before** any model sees it.\nThe cleaned text is what both models operate on, so the comparison is fair.\n\n| # | Step | Example effect |\n|---|------|----------------|\n| 1 | Lower-case | `FREE iPhone` \u2192 `free iphone` |\n| 2 | Token substitution | URLs \u2192 `url` \u00b7 phones \u2192 `phone` \u00b7 `$500` \u2192 `money` |\n| 3 | Strip punctuation | `click here!` \u2192 `click here` |\n| 4 | Tokenise | Split on whitespace |\n| 5 | Remove stop-words | Drop *the, is, a, \u2026* and single-char tokens |\n| 6 | Porter stem | `running` \u2192 `run` \u00b7 `prizes` \u2192 `prize` |\n"
  },
  {
   "id": "cell-08",
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from preprocess import clean_text\n\nexamples = [\n    \"Congratulations! You have WON a FREE iPhone! Click https://scam.com/prize now!\",\n    \"Hey, are we still meeting at 3pm? Call me at 555-123-4567 if not.\",\n    \"URGENT: You owe $500 in back taxes. Pay immediately or face penalties!\",\n    \"Thanks for sending the Q3 report. I'll review it by Friday.\",\n]\n\npd.DataFrame({\n    \"Original\": examples,\n    \"Cleaned\" : [clean_text(t) for t in examples],\n})\n"
  },
  {
   "id": "cell-09",
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 3  Baseline \u2014 TF-IDF + Logistic Regression\n\n**TfidfVectorizer** converts cleaned text into a sparse numeric matrix\n(up to 50 000 uni + bigram features, sub-linear TF scaling).\n\n**LogisticRegression** with `class_weight='balanced'` compensates for the\n13 / 87 class imbalance by up-weighting the minority (spam) class internally.\n"
  },
  {
   "id": "cell-10",
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.linear_model import LogisticRegression\nfrom preprocess import preprocess_df\nfrom config import (SEED, TFIDF_MAX_FEATURES, TFIDF_MIN_DF,\n                    TFIDF_NGRAM_RANGE, TFIDF_SUBLINEAR_TF, LR_C, LR_MAX_ITER)\n\n# Preprocess all three splits\ntrain_p = preprocess_df(train_df)\nval_p   = preprocess_df(val_df)\ntest_p  = preprocess_df(test_df)\n\n# TF-IDF \u2014 fit on training data only\ntfidf = TfidfVectorizer(\n    max_features=TFIDF_MAX_FEATURES,\n    ngram_range=TFIDF_NGRAM_RANGE,\n    sublinear_tf=TFIDF_SUBLINEAR_TF,\n    min_df=TFIDF_MIN_DF,\n)\nX_train = tfidf.fit_transform(train_p[\"cleaned_text\"])\nX_val   = tfidf.transform(val_p[\"cleaned_text\"])\nX_test  = tfidf.transform(test_p[\"cleaned_text\"])\n\ny_train = train_p[\"label\"].values\ny_val   = val_p[\"label\"].values\ny_test  = test_p[\"label\"].values\n\nprint(f\"Feature matrix: {X_train.shape[0]} samples x {X_train.shape[1]} features\")\n\n# Logistic Regression\nlr_model = LogisticRegression(\n    C=LR_C, max_iter=LR_MAX_ITER,\n    class_weight=\"balanced\", solver=\"lbfgs\", random_state=SEED,\n)\nlr_model.fit(X_train, y_train)\nprint(\"Logistic Regression trained.\")\n"
  },
  {
   "id": "cell-11",
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from sklearn.metrics import classification_report\nfrom evaluate import compute_metrics\n\n# Validation\nprint(\"=\" * 50)\nprint(\"  Validation\")\nprint(\"=\" * 50)\nprint(classification_report(y_val, lr_model.predict(X_val), target_names=[\"Ham\", \"Spam\"]))\n\n# Test\ntest_preds_lr  = lr_model.predict(X_test)\ntest_probs_lr  = lr_model.predict_proba(X_test)[:, 1]\nbaseline_metrics = compute_metrics(y_test, test_preds_lr, test_probs_lr)\n\nprint(\"=\" * 50)\nprint(\"  Test Set\")\nprint(\"=\" * 50)\nprint(classification_report(y_test, test_preds_lr, target_names=[\"Ham\", \"Spam\"]))\n\npd.DataFrame(baseline_metrics, index=[0]).T.rename(columns={0: \"Score\"}).round(4)\n"
  },
  {
   "id": "cell-12",
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from sklearn.metrics import confusion_matrix, roc_curve, auc\n\nfig, axes = plt.subplots(1, 2, figsize=(12, 4.5))\n\n# Confusion matrix\nsns.heatmap(\n    confusion_matrix(y_test, test_preds_lr),\n    annot=True, fmt=\"d\", cmap=\"Blues\", ax=axes[0],\n    xticklabels=[\"Ham\", \"Spam\"], yticklabels=[\"Ham\", \"Spam\"],\n    cbar=False, linewidths=1, linecolor=\"black\",\n)\naxes[0].set_xlabel(\"Predicted\")\naxes[0].set_ylabel(\"Actual\")\naxes[0].set_title(\"Confusion Matrix \u2013 Baseline\", fontsize=13)\n\n# ROC\nfpr, tpr, _ = roc_curve(y_test, test_probs_lr)\naxes[1].plot(fpr, tpr, color=\"darkorange\", lw=2, label=f\"AUC = {auc(fpr, tpr):.3f}\")\naxes[1].plot([0, 1], [0, 1], color=\"navy\", lw=1, linestyle=\"--\")\naxes[1].set_xlabel(\"False Positive Rate\")\naxes[1].set_ylabel(\"True Positive Rate\")\naxes[1].set_title(\"ROC Curve \u2013 Baseline\", fontsize=13)\naxes[1].legend(loc=\"lower right\", fontsize=11)\naxes[1].grid(alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n"
  },
  {
   "id": "cell-13",
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 4  Advanced \u2014 Bi-directional LSTM\n\nUnlike the bag-of-words baseline, the LSTM **learns its own word embeddings** and\nprocesses tokens **sequentially**, letting it capture word-order patterns.\n\n| Component | Details |\n|-----------|---------|\n| Embedding | 128-dim, learned, PAD-masked |\n| BiLSTM | 1 layer \u00b7 128 hidden units per direction |\n| Dropout | 0.3 \u2014 after embedding & before the FC layer |\n| Output | Linear(256 \u2192 1) raw logit; sigmoid \u2192 P(spam) |\n| Loss | `BCEWithLogitsLoss` with `pos_weight = ham_count / spam_count` |\n| Optimiser | Adam (lr = 0.001), gradient clipping @ 1.0 |\n\n**Best checkpoint:** the model state with the highest validation macro-F1\nis kept and loaded before the final test evaluation.\n"
  },
  {
   "id": "cell-14",
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import torch\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader\nfrom train_lstm import SpamLSTM, Vocabulary, SpamDataset, train_one_epoch, eval_epoch\nfrom config import VOCAB_SIZE, BATCH_SIZE, EPOCHS, LSTM_LR\n\ntorch.manual_seed(SEED)\nnp.random.seed(SEED)\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Device: {device}\")\n\n# Vocabulary \u2014 built on training text only\nvocab = Vocabulary(max_size=VOCAB_SIZE)\nvocab.build(train_p[\"cleaned_text\"].tolist())\nprint(f\"Vocabulary size: {len(vocab):,}\")\n\n# DataLoaders\ndef _ds(df):\n    return SpamDataset(df[\"cleaned_text\"].tolist(), df[\"label\"].tolist(), vocab)\n\ntrain_loader = DataLoader(_ds(train_p), batch_size=BATCH_SIZE, shuffle=True)\nval_loader   = DataLoader(_ds(val_p),   batch_size=BATCH_SIZE)\ntest_loader  = DataLoader(_ds(test_p),  batch_size=BATCH_SIZE)\n\n# pos_weight to compensate for class imbalance\nn_spam     = int(train_p[\"label\"].sum())\npos_weight = torch.tensor([(len(train_p) - n_spam) / n_spam],\n                          dtype=torch.float, device=device)\nprint(f\"pos_weight = {pos_weight.item():.2f}\")\n\n# Model / optimiser / loss\nlstm_model = SpamLSTM(vocab_size=len(vocab)).to(device)\noptimizer  = torch.optim.Adam(lstm_model.parameters(), lr=LSTM_LR)\ncriterion  = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n\n# Training loop with best-checkpoint tracking\nhistory            = {\"train_loss\": [], \"val_loss\": [], \"val_f1\": []}\nbest_f1, best_state = 0.0, None\n\nprint(f\"\\nTraining for {EPOCHS} epochs ...\")\nfor epoch in range(1, EPOCHS + 1):\n    t_loss = train_one_epoch(lstm_model, train_loader, optimizer, criterion, device)\n    v_loss, v_preds, v_probs, v_labels = eval_epoch(\n        lstm_model, val_loader, criterion, device\n    )\n    v_f1 = compute_metrics(v_labels, v_preds, v_probs)[\"f1_macro\"]\n\n    history[\"train_loss\"].append(t_loss)\n    history[\"val_loss\"].append(v_loss)\n    history[\"val_f1\"].append(v_f1)\n\n    marker = \"\"\n    if v_f1 > best_f1:\n        best_f1    = v_f1\n        best_state = {k: v.clone() for k, v in lstm_model.state_dict().items()}\n        marker     = \"  <- best\"\n\n    print(f\"  Epoch {epoch:>2}/{EPOCHS}  \"\n          f\"train_loss={t_loss:.4f}  \"\n          f\"val_loss={v_loss:.4f}  \"\n          f\"val_f1={v_f1:.4f}{marker}\")\n\n# Restore best checkpoint\nlstm_model.load_state_dict(best_state)\nprint(f\"\\nBest validation F1: {best_f1:.4f}\")\n"
  },
  {
   "id": "cell-15",
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\nep = range(1, len(history[\"train_loss\"]) + 1)\n\naxes[0].plot(ep, history[\"train_loss\"], \"b-o\", markersize=4, label=\"Train\")\naxes[0].plot(ep, history[\"val_loss\"],   \"r-o\", markersize=4, label=\"Val\")\naxes[0].set_xlabel(\"Epoch\")\naxes[0].set_ylabel(\"Loss\")\naxes[0].set_title(\"Training & Validation Loss\", fontsize=13)\naxes[0].legend()\naxes[0].grid(alpha=0.3)\n\naxes[1].plot(ep, history[\"val_f1\"], \"g-o\", markersize=4)\naxes[1].axhline(best_f1, color=\"green\", ls=\"--\", alpha=0.5,\n                label=f\"best = {best_f1:.4f}\")\naxes[1].set_xlabel(\"Epoch\")\naxes[1].set_ylabel(\"F1 (macro)\")\naxes[1].set_title(\"Validation F1 Score\", fontsize=13)\naxes[1].legend()\naxes[1].grid(alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n"
  },
  {
   "id": "cell-16",
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "_, test_preds_lstm, test_probs_lstm, _ = eval_epoch(\n    lstm_model, test_loader, criterion, device\n)\n\nprint(\"=\" * 50)\nprint(\"  Test Set - Bi-LSTM\")\nprint(\"=\" * 50)\nprint(classification_report(y_test, test_preds_lstm, target_names=[\"Ham\", \"Spam\"]))\n\nlstm_metrics = compute_metrics(y_test, test_preds_lstm, test_probs_lstm)\npd.DataFrame(lstm_metrics, index=[0]).T.rename(columns={0: \"Score\"}).round(4)\n"
  },
  {
   "id": "cell-17",
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "fig, axes = plt.subplots(1, 2, figsize=(12, 4.5))\n\n# Confusion matrix\nsns.heatmap(\n    confusion_matrix(y_test, test_preds_lstm),\n    annot=True, fmt=\"d\", cmap=\"Blues\", ax=axes[0],\n    xticklabels=[\"Ham\", \"Spam\"], yticklabels=[\"Ham\", \"Spam\"],\n    cbar=False, linewidths=1, linecolor=\"black\",\n)\naxes[0].set_xlabel(\"Predicted\")\naxes[0].set_ylabel(\"Actual\")\naxes[0].set_title(\"Confusion Matrix \u2013 Bi-LSTM\", fontsize=13)\n\n# ROC\nfpr, tpr, _ = roc_curve(y_test, test_probs_lstm)\naxes[1].plot(fpr, tpr, color=\"darkorange\", lw=2, label=f\"AUC = {auc(fpr, tpr):.3f}\")\naxes[1].plot([0, 1], [0, 1], color=\"navy\", lw=1, linestyle=\"--\")\naxes[1].set_xlabel(\"False Positive Rate\")\naxes[1].set_ylabel(\"True Positive Rate\")\naxes[1].set_title(\"ROC Curve \u2013 Bi-LSTM\", fontsize=13)\naxes[1].legend(loc=\"lower right\", fontsize=11)\naxes[1].grid(alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n"
  },
  {
   "id": "cell-18",
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 5  Model Comparison\n\nBoth models were evaluated on the **same held-out test set** (15 % of the data).\nThe chart and table below give a side-by-side view of every metric.\n"
  },
  {
   "id": "cell-19",
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "KEYS   = [\"accuracy\", \"precision_macro\", \"recall_macro\", \"f1_macro\", \"auc\"]\nCHART  = [\"Accuracy\", \"Precision\\n(macro)\", \"Recall\\n(macro)\", \"F1\\n(macro)\", \"AUC-ROC\"]\nTABLE  = [\"Accuracy\", \"Precision (macro)\", \"Recall (macro)\", \"F1 (macro)\", \"AUC-ROC\"]\n\nx, w = np.arange(len(KEYS)), 0.33\n\nfig, ax = plt.subplots(figsize=(10, 5))\nbars_b = ax.bar(x - w/2, [baseline_metrics[k] for k in KEYS], w,\n                label=\"TF-IDF + LR\", color=\"steelblue\", edgecolor=\"white\")\nbars_l = ax.bar(x + w/2, [lstm_metrics[k]     for k in KEYS], w,\n                label=\"Bi-LSTM\",     color=\"coral\",    edgecolor=\"white\")\n\nfor bars in (bars_b, bars_l):\n    for bar in bars:\n        ax.text(bar.get_x() + bar.get_width() / 2,\n                bar.get_height() + 0.004,\n                f\"{bar.get_height():.3f}\",\n                ha=\"center\", va=\"bottom\", fontsize=8.5)\n\nax.set_xticks(x)\nax.set_xticklabels(CHART, fontsize=10)\nax.set_ylabel(\"Score\", fontsize=11)\nax.set_title(\"Model Comparison \u2013 Test Set\", fontsize=13)\nax.legend(fontsize=10)\nax.set_ylim(0.80, 1.04)\nax.grid(axis=\"y\", alpha=0.3)\nplt.tight_layout()\nplt.show()\n\n# Summary table\npd.DataFrame({\n    \"Metric\":     TABLE,\n    \"TF-IDF+LR\":  [round(baseline_metrics[k], 4) for k in KEYS],\n    \"Bi-LSTM\":    [round(lstm_metrics[k],     4) for k in KEYS],\n}).set_index(\"Metric\")\n"
  },
  {
   "id": "cell-20",
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 6  Live Predictions\n\nClassify hand-written sample emails with both trained models.\n*Confidence* is the probability the model assigns to its chosen label.\n"
  },
  {
   "id": "cell-21",
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "sample_emails = [\n    \"Congratulations! You have won a free iPhone! Click here to claim your prize immediately.\",\n    \"Hey, are we still meeting for dinner tonight? Let me know what time works.\",\n    \"URGENT: Your account has been suspended. Click the link below to verify now.\",\n    \"Thanks for sending the meeting notes. I will review them by end of day.\",\n    \"You owe $500 in back taxes. Call 555-123-4567 immediately to avoid penalties.\",\n    \"Just checking in \u2013 hope your weekend was great! See you Monday.\",\n    \"Win a FREE vacation to the Bahamas! Reply YES now to claim your reward.\",\n    \"Can you send me the quarterly report when you get a chance? No rush.\",\n]\n\ncleaned = [clean_text(t) for t in sample_emails]\n\n# Baseline\nX_sample  = tfidf.transform(cleaned)\nbl_preds  = lr_model.predict(X_sample)\nbl_probs  = lr_model.predict_proba(X_sample)[:, 1]\n\n# Bi-LSTM\nids_t = torch.tensor([vocab.encode(t) for t in cleaned], dtype=torch.long)\nwith torch.no_grad():\n    lstm_probs_s = torch.sigmoid(lstm_model(ids_t.to(device))).cpu().numpy()\nlstm_preds_s = (lstm_probs_s > 0.5).astype(int)\n\n# Results table\ndef _lbl(p):  return \"SPAM\" if p else \"HAM\"\ndef _conf(p): return f\"{max(float(p), 1 - float(p)):.1%}\"\n\npd.DataFrame({\n    \"Email\":           [t[:58] + (\"...\" if len(t) > 58 else \"\") for t in sample_emails],\n    \"Baseline\":        [_lbl(p) for p in bl_preds],\n    \"BL Confidence\":   [_conf(p) for p in bl_probs],\n    \"Bi-LSTM\":         [_lbl(p) for p in lstm_preds_s],\n    \"LSTM Confidence\": [_conf(p) for p in lstm_probs_s],\n})\n"
  },
  {
   "id": "cell-22",
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Summary\n\n| | TF-IDF + LR | Bi-LSTM |\n|---|---|---|\n| **Approach** | Bag-of-words features + linear classifier | Learned embeddings + sequential model |\n| **Strengths** | Fast, interpretable, strong on keyword-heavy spam | Captures word order and context |\n| **Best for** | Short messages where key words dominate | Longer or more nuanced text |\n\nBoth models perform well on this dataset. The TF-IDF baseline is competitive because\nspam emails tend to rely on distinctive keywords \u2014 exactly what TF-IDF captures.\nThe Bi-LSTM has an edge when sequential context matters, which becomes more\nimportant on longer or more subtle messages.\n\n---\n\n*All hyper-parameters live in `config.py`. To retrain with different settings,\nchange a value there and re-run the relevant cell.*\n"
  }
 ]
}